# CNN (Convolutional Neural Network)
## Convolutional Layer
- A neural network with some convolutional layers. A convolutional layer has a number of filters that does convolutional operation
- Each convolution filter detects a small pattern
- Strides and checks where there are matches (dot product matches)
- Fully connected, all the values in the original image are connected, in a fully connected we will use all values
## Max Pooling
- Get maximum value in a pool, to reduce the size of the image (subsampling), gives us the highest response in an area that we can use so we can get rid of the rest
- Subsampling pixels will not change the object, we can subsample the pixels to make it smaller, fewer parameters to characterise the image
- Compresses a fully connected network
	- Reducing number of connections
	- Shared weights on the edges
	- Max pooling further reduces complexity
- 3x3 filter subtracts 2, etc
```python
# 1x28x28
# 9 parameters
model2.add(Convolution2D(25,3,3,input_shape=(28,28,1))) 
# 25x26x26
model2.add(MaxPooling2D(2,2))
# 25x13x13
model2.add(Convolution2(50,3,3))
# 50x11x11, paras = 25 * 9 = 225
model2.add(MaxPooling2D((2,2)))
# 50x5x5
model2.add(Flatten())
# 1250

# Fully Connected Network
model2.add(Dense(output_dim=100)) #reduce to 100 neurons
model2.add(Activation('relu')) #relu will make anything <0 equal to 0
model2.add(Dense(output_dim=10)) #reduce to 10 neurons
model2.add(Activation('softmax')) #give probability
```
## What do CNN filters learn
- Initial convolution layers -> deeper convolution layers
- Low level features -> mid level features -> high level features
- Visualisations of feature maps at different layers in a CNN
- Final output at the rightmost indicates the fully connected layer's output
## Batch Normalisation
- Consider a single layer $y=Wx$
- Following leads to tough optimisation
	- Inputs $x$ are not centred around zero
	- Inputs $x$ have different scaling per-element (entries in $W$ will need to vary a lot)
	- THEREFORE force inputs to be nicely scaled at each layer
- Make the inputs have zero mean and unit variance
# AlexNet
```
Input: 227x227x3 images
First layer (CONV1):
	96 11x11 filters applied at stride 4
Output volume size = 55x55x96
Total no. parameters in this layer = (11*11*3+1)*96 = 35k
Second layer (POOL1):
	3x3 filters applied at stride 2
Output volume size = 27x27x96
Total no. parameters in this layer = 0
```
