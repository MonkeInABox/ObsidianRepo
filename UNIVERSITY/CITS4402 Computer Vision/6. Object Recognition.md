## Recognition: Detection vs. Recognition
- Are there faces in the image?
- Where are faces in this image? (face detection)
- Is your person of interest present in this image? (again binary decision)
- Where is your person of interest? (face recognition)
- What are these people doing? (activity or event recognition)

## Scale of Detection
- Faces may need to be detected at different scales
- We can have nested detection
	- Detect face
	- Detect features such as eye corners, nose tip, etc.

## Semantic Vision
- In general, semantics concerns with the extraction of meaning from data, seeks to understand not only what objects are present in an image but the relationship between those objects
- The ability to attribute relationships between objects demonstrates reasoning

## Image Features
- Two primary characteristics for object recognition: shape and appearance
- Shape can be modelled with Principal Component Analysis (PCA)
- PCA can also model appearance
- Appearance can be modelled with Colour Histograms
- How does PCA work?
	- Centre the data
		- Centre or standardise the data in the first step of the PCA
		- Subtract all the values for each variable by its corresponding mean
	- Calculate covariance matrix
	- Calculate the eigenvalues
	- Calculate the eigenvectors
	- Order the eigenvectors
	- Calculate the principal components
- Maths:
	- Let X be a matrix of the training images
	- Find its column mean $$\mu = \frac{1}{n}\Sigma X_{i}(averageFace)$$
	- Subtract mean from all data $$\hat{X}=X-\mu$$
	- Find the covariance matrix $$USV^T=\hat{X}\hat{X}^T$$
	- Columns of U are the eigenvectors and diagonal of S are the eigenvalues 
	- Choose P eigenvectors 
	- If $m$ is the number of pixels and $m > n$ then use the following trick $$U_1SV^T=\hat{X}^T\hat{X}$$ and $$U = \hat{X}U_1$$
	- $U$ (the eigenvector matrix) is calculated from training data
		- Training data $X$ is projected to the PCA space using $U$
		- Test data is also projected to the same PCA space
		- Nearest neighbour is used for classification
		- If the original images were of $m\times x=(50)(50)=2500$ dimension and we chose $P=20$. The projected images will be only 20 dimensional
		- If our training samples $n=100$, the total possible eigenvectors within non-zero eigenvalues will always be < 100 (99 at most)
## Training
1. Align training images
2. Compute average face $\mu = \frac{1}{N}\Sigma x_i$
3. Compute PCA of the covariance matrices of the different images
4. Compute training projections $a_{1}, a_{2}, ..., a_N$
## Testing
- Visualisation of eigenfaces
- These are the first 4 eigenfaces (eigenvectors) from a training set of 400 images
1. Take query image $y$
2. Project $y$ into the eigenface space $\omega=U^T_p(y-\mu)$
3. Compare projection $\omega$ with all training projection $a_i$
4. Identity of the query image $X$ is chosen as that of the nearest 
## Colour Histogram
- Stays constant under geometric transformations
- Colour is a local feature
	- It is defined for each pixel
	- It is robust to partial occlusion
- Idea: 
	- Can use object colours directly for recognition
	- Better
## Object Recognition Using Histograms
1. Build a set of histograms $H=\{h_i\}$ for each known object
	- For each view of each object
2. Build a histogram $h_t$ for the test image
3. Compare $h_t$ with each $h_{t} \in H$ using a suitable histogram comparison measure
4. Select the object with the best matching score